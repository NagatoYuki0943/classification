{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://huggingface.co/docs/transformers/main/en/model_doc/ul2\n",
    "\n",
    "\n",
    "The T5 model was presented in Unifying Language Learning Paradigms by Yi Tay, Mostafa Dehghani, Vinh Q. Tran, Xavier Garcia, Dara Bahri, Tal Schuster, Huaixiu Steven Zheng, Neil Houlsby, Donald Metzler.\n",
    "\n",
    "Tips:\n",
    "- UL2 is an encoder-decoder model pre-trained on a mixture of denoising functions as well as fine-tuned on an array of downstream tasks.\n",
    "- UL2 has the same architecture as [T5v1.1](https://huggingface.co/docs/transformers/main/en/model_doc/t5v1.1) but uses the Gated-SiLU activation function instead of Gated-GELU.\n",
    "- The authors release checkpoints of one architecture which can be seen [here](https://github.com/google-research/google-research/tree/master/ul2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer, GenerationConfig\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = \"google/flan-ul2\" # google/ul2\n",
    "encoder_input = \"Studies have been shown that owning a dog is good for you\"\n",
    "decoder_input = \"Studies show that\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T5Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer: T5Tokenizer = T5Tokenizer.from_pretrained(version)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tokenizer([sequence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.tokenize(encoder_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_inputs = tokenizer(\n",
    "    encoder_input,                      # 句子batch\n",
    "    truncation = True,                  # 超出max_length截断处理\n",
    "    padding = True,                     # 填充方式选择 [True, 'longest', 'max_length', 'do_not_pad']\n",
    "    # max_length = max_length,          # 最长长度,不设置默认为模型最大长度\n",
    "    add_special_tokens = True,          # text添加特殊key\n",
    "    return_length = True,               # 返回有效长度\n",
    "    return_overflowing_tokens = False,  # 返回所有的文本片段（由于文本比较长，默认情况下超过预设截断长度的token会被丢失。如果设置了return_overflowing_tokens=True则会返回所有的token片段）。\n",
    "    return_tensors = \"pt\"               # 返回数据格式 np pt tf jax\n",
    ").to(device)\n",
    "\n",
    "print(encoder_inputs.keys())\n",
    "print(encoder_inputs[\"input_ids\"])\n",
    "print(encoder_inputs[\"attention_mask\"]) # 对应是否是文字\n",
    "print(encoder_inputs[\"length\"])         # 对应有效文字长度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.tokenize(decoder_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_inputs = tokenizer(\n",
    "    decoder_input,                      # 句子batch\n",
    "    truncation = True,                  # 超出max_length截断处理\n",
    "    padding = True,                     # 填充方式选择 [True, 'longest', 'max_length', 'do_not_pad']\n",
    "    # max_length = max_length,          # 最长长度,不设置默认为模型最大长度\n",
    "    add_special_tokens = True,          # text添加特殊key\n",
    "    return_length = True,               # 返回有效长度\n",
    "    return_overflowing_tokens = False,  # 返回所有的文本片段（由于文本比较长，默认情况下超过预设截断长度的token会被丢失。如果设置了return_overflowing_tokens=True则会返回所有的token片段）。\n",
    "    return_tensors = \"pt\"               # 返回数据格式 np pt tf jax\n",
    ").to(device)\n",
    "\n",
    "print(decoder_inputs.keys())\n",
    "print(decoder_inputs[\"input_ids\"])\n",
    "print(decoder_inputs[\"attention_mask\"]) # 对应是否是文字\n",
    "print(decoder_inputs[\"length\"])         # 对应有效文字长度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T5ForConditionalGeneration\n",
    "\n",
    "T5 Model with a language modeling head on top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model: T5ForConditionalGeneration = T5ForConditionalGeneration.from_pretrained(version).to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.inference_mode():\n",
    "    # 可以只输入input而不输入decoder\n",
    "    outputs = model.generate(\n",
    "        input_ids = encoder_inputs[\"input_ids\"],\n",
    "        attention_mask = encoder_inputs[\"attention_mask\"],\n",
    "        decoder_input_ids = decoder_inputs[\"input_ids\"],\n",
    "        decoder_attention_mask = decoder_inputs[\"attention_mask\"],\n",
    "        # num_beams: beam search num\n",
    "        generation_config = GenerationConfig(num_beams=1, min_length=0, max_new_tokens=100),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.encode(decoder_input))\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(decoder_input)\n",
    "print(tokenizer.batch_decode(outputs, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.batch_decode([0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 可以只输入input而不输入decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(\"A step by step recipe to make bolognese pasta:\", return_tensors=\"pt\").to(device)\n",
    "with torch.inference_mode():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        # num_beams: beam search num\n",
    "        generation_config = GenerationConfig(num_beams=1, min_length=0, max_new_tokens=100),\n",
    "    )\n",
    "print(outputs)\n",
    "print(tokenizer.batch_decode(outputs, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(version)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoModelForSeq2SeqLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(version).to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(\"A step by step recipe to make bolognese pasta:\", return_tensors=\"pt\").to(device)\n",
    "model.eval()\n",
    "with torch.inference_mode():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        # num_beams: beam search num\n",
    "        generation_config = GenerationConfig(num_beams=1, min_length=0, max_new_tokens=100),\n",
    "    )\n",
    "print(outputs)\n",
    "print(tokenizer.batch_decode(outputs, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
