{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://huggingface.co/docs/accelerate\n",
    "\n",
    "https://huggingface.co/docs/accelerate/quicktour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from accelerate import Accelerator\n",
    "from torchmetrics.functional import accuracy\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "batch_size = 100\n",
    "in_features = 10\n",
    "num_classes = 5\n",
    "data_len = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Linear(in_features, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=0.001,\n",
    "    betas=(0.9, 0.999),\n",
    "    weight_decay=1e-2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_sche = optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer, T_max=epochs, eta_min=0.001 * 100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(Dataset):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.x = torch.randn(data_len, in_features)\n",
    "        self.y = torch.randint(0, num_classes, (data_len,))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datalaoder = DataLoader(\n",
    "    dataset=Dataset(),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    ")\n",
    "val_datalaoder = DataLoader(\n",
    "    dataset=Dataset(),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accelerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('accelerator_checkpoint')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dir = Path(\"accelerator_checkpoint\")\n",
    "output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "accelerator = Accelerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accelerator.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('weight',\n",
       "              tensor([[-0.0582, -0.1446, -0.2358, -0.1155,  0.2035, -0.2924, -0.2298,  0.0446,\n",
       "                        0.0732, -0.3009],\n",
       "                      [ 0.1364, -0.2627, -0.2545, -0.2182,  0.2711,  0.2517,  0.2459,  0.0006,\n",
       "                        0.1983, -0.2905],\n",
       "                      [ 0.1668,  0.2241, -0.2459,  0.1288,  0.0057,  0.1226,  0.1944,  0.1659,\n",
       "                       -0.1588,  0.1786],\n",
       "                      [ 0.1793, -0.1016,  0.3151,  0.0429, -0.0895,  0.0295, -0.1313, -0.0883,\n",
       "                        0.1461, -0.1601],\n",
       "                      [-0.3099, -0.1955,  0.0575, -0.0787,  0.3042,  0.2530, -0.0462,  0.2949,\n",
       "                       -0.0340,  0.0074]])),\n",
       "             ('bias', tensor([ 0.1206, -0.0559, -0.1660, -0.1939,  0.0456]))])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# accelerator可以帮助获取state\n",
    "accelerator.get_state_dict(model, unwrap=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('weight',\n",
       "              tensor([[-0.0582, -0.1446, -0.2358, -0.1155,  0.2035, -0.2924, -0.2298,  0.0446,\n",
       "                        0.0732, -0.3009],\n",
       "                      [ 0.1364, -0.2627, -0.2545, -0.2182,  0.2711,  0.2517,  0.2459,  0.0006,\n",
       "                        0.1983, -0.2905],\n",
       "                      [ 0.1668,  0.2241, -0.2459,  0.1288,  0.0057,  0.1226,  0.1944,  0.1659,\n",
       "                       -0.1588,  0.1786],\n",
       "                      [ 0.1793, -0.1016,  0.3151,  0.0429, -0.0895,  0.0295, -0.1313, -0.0883,\n",
       "                        0.1461, -0.1601],\n",
       "                      [-0.3099, -0.1955,  0.0575, -0.0787,  0.3042,  0.2530, -0.0462,  0.2949,\n",
       "                       -0.0340,  0.0074]])),\n",
       "             ('bias', tensor([ 0.1206, -0.0559, -0.1660, -0.1939,  0.0456]))])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## is_main_process is_local_main_process\n",
    "\n",
    "is_local_main_process 和 is_main_process 就跟Pytorch的分布式训练中的LOCAL_RANK和RANK的区别"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> node\n",
    "物理节点，就是一台机器，节点内部可以有多个GPU(一台机器有多卡)。\n",
    "\n",
    "> rank & local_rank\n",
    ">\n",
    "> 用于表示进程的序号，用于进程间通信。每一个进程对应了一个rank。\n",
    ">\n",
    "> rank=0的进程就是master进程。\n",
    ">\n",
    "> local_rank： rank是指在整个分布式任务中进程的序号；local_rank是指在一台机器上(一个node上)进程的相对序号，例如机器一上有0,1,2,3,4,5,6,7，机器二上也有0,1,2,3,4,5,6,7。local_rank在node之间相互独立。\n",
    ">\n",
    "> 单机多卡时，rank就等于local_rank\n",
    "\n",
    "> nnodes\n",
    ">\n",
    "> 物理节点数量\n",
    "\n",
    "> node_rank\n",
    ">\n",
    "> 物理节点的序号\n",
    "\n",
    "> nproc_per_node\n",
    ">\n",
    "> 每个物理节点上面进程的数量。\n",
    "\n",
    "> group\n",
    ">\n",
    "> 进程组。默认只有一个组\n",
    "\n",
    "> world size 全局的并行数\n",
    ">\n",
    "> 全局（一个分布式任务）中，rank的数量。\n",
    ">\n",
    "> 每个node包含16个GPU，且nproc_per_node=8，nnodes=3，机器的node_rank=5，请问world_size是多少？\n",
    ">\n",
    "> 答案：world_size = 3*8 = 24\n",
    "\n",
    "\n",
    "```yaml\n",
    "# 一共有12个rank, nnodes=3, nproc_per_node=4,每个节点都对应一个node_rank\n",
    "\n",
    "machine0:\n",
    "    node_rank: 0\n",
    "        GPU0:\n",
    "            rank: 0\n",
    "            local_rank: 0\n",
    "        GPU1:\n",
    "            rank: 1\n",
    "            local_rank: 1\n",
    "        GPU2:\n",
    "            rank: 2\n",
    "            local_rank: 2\n",
    "        GPU3:\n",
    "            rank: 3\n",
    "            local_rank: 3\n",
    "\n",
    "machine1:\n",
    "    node_rank: 1\n",
    "        GPU0:\n",
    "            rank: 4\n",
    "            local_rank: 0\n",
    "        GPU1:\n",
    "            rank: 5\n",
    "            local_rank: 1\n",
    "        GPU2:\n",
    "            rank: 6\n",
    "            local_rank: 2\n",
    "        GPU3:\n",
    "            rank: 7\n",
    "            local_rank: 3\n",
    "\n",
    "machine2:\n",
    "    node_rank: 2\n",
    "        GPU0:\n",
    "            rank: 8\n",
    "            local_rank: 0\n",
    "        GPU1:\n",
    "            rank: 9\n",
    "            local_rank: 1\n",
    "        GPU2:\n",
    "            rank: 10\n",
    "            local_rank: 2\n",
    "        GPU3:\n",
    "            rank: 11\n",
    "            local_rank: 3\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(accelerator.is_main_process)\n",
    "print(accelerator.is_local_main_process)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, optimizer, train_datalaoder, val_datalaoder = accelerator.prepare(\n",
    "    model, optimizer, train_datalaoder, val_datalaoder\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## clip gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accelerator.clip_grad_norm_(parameters=model.parameters(), max_norm=1, norm_type=2)\n",
    "torch.nn.utils.clip_grad.clip_grad_norm_(\n",
    "    parameters=model.parameters(), max_norm=1, norm_type=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accelerator.clip_grad_value_(parameters=model.parameters(), clip_value=0.1)\n",
    "# torch.nn.utils.clip_grad.clip_grad_value_(parameters=model.parameters(), clip_value=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1/5: 100%|██████████| 100/100 [00:00<00:00, 201.79it/s, train/acc=0.1976]\n",
      "1/5:  69%|██████▉   | 69/100 [00:00<00:00, 600.00it/s, val/loss=1.6828]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1/5: 100%|██████████| 100/100 [00:00<00:00, 581.40it/s, val/acc=0.2001]\n",
      "2/5: 100%|██████████| 100/100 [00:00<00:00, 318.47it/s, train/acc=0.1975]\n",
      "2/5: 100%|██████████| 100/100 [00:00<00:00, 588.24it/s, val/acc=0.1966]\n",
      "3/5: 100%|██████████| 100/100 [00:00<00:00, 350.18it/s, train/acc=0.1984]\n",
      "3/5: 100%|██████████| 100/100 [00:00<00:00, 598.81it/s, val/acc=0.1958]\n",
      "4/5: 100%|██████████| 100/100 [00:00<00:00, 346.81it/s, train/acc=0.2063]\n",
      "4/5: 100%|██████████| 100/100 [00:00<00:00, 575.92it/s, val/acc=0.1910]\n",
      "5/5: 100%|██████████| 100/100 [00:00<00:00, 337.69it/s, train/acc=0.1936]\n",
      "5/5: 100%|██████████| 100/100 [00:00<00:00, 572.85it/s, val/acc=0.1925]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training finish\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, epochs + 1):\n",
    "    # train\n",
    "    model.train()\n",
    "    with tqdm(\n",
    "        total=len(train_datalaoder),\n",
    "        desc=f\"{epoch}/{epochs}\",\n",
    "        disable=not accelerator.is_main_process,\n",
    "    ) as pbar:\n",
    "        all_predictions = []\n",
    "        all_targets = []\n",
    "        all_losses = []\n",
    "        for x, y in train_datalaoder:\n",
    "            optimizer.zero_grad()\n",
    "            with accelerator.autocast():\n",
    "                y_pred: torch.Tensor = model(x)\n",
    "                loss: torch.Tensor = loss_fn(y_pred, y)\n",
    "            accelerator.backward(loss)  # replace loss.backward()\n",
    "            accelerator.clip_grad_norm_(  # 梯度裁剪\n",
    "                parameters=model.parameters(),\n",
    "                max_norm=1.0,\n",
    "                norm_type=2,\n",
    "            )\n",
    "            optimizer.step()\n",
    "\n",
    "            pbar.set_postfix({\"train/loss\": f\"{loss.item():.4f}\"})\n",
    "            pbar.update(1)\n",
    "\n",
    "            # 获取所有数据上的预测值和真实值,用来验证\n",
    "            all_pred, all_tar, all_loss = accelerator.gather_for_metrics(\n",
    "                (y_pred, y, loss)\n",
    "            )\n",
    "            all_predictions.append(all_pred)\n",
    "            all_targets.append(all_tar)\n",
    "            all_losses.append(all_loss)\n",
    "\n",
    "        train_acc = accuracy(\n",
    "            preds=torch.cat(all_predictions, dim=0),\n",
    "            target=torch.cat(all_targets, dim=0),\n",
    "            task=\"multiclass\",\n",
    "            num_classes=num_classes,\n",
    "        )\n",
    "        train_avg_loss = torch.mean(torch.tensor(all_losses))\n",
    "        pbar.set_postfix({\"train/acc\": f\"{train_acc.item():.4f}\"})\n",
    "\n",
    "    lr_sche.step()\n",
    "\n",
    "    # val\n",
    "    model.eval()\n",
    "    with tqdm(\n",
    "        total=len(val_datalaoder),\n",
    "        desc=f\"{epoch}/{epochs}\",\n",
    "        disable=not accelerator.is_main_process,\n",
    "    ) as pbar:\n",
    "        all_predictions = []\n",
    "        all_targets = []\n",
    "        all_losses = []\n",
    "        for x, y in val_datalaoder:\n",
    "            with torch.inference_mode():\n",
    "                y_pred: torch.Tensor = model(x)\n",
    "            loss: torch.Tensor = loss_fn(y_pred, y)\n",
    "\n",
    "            pbar.set_postfix({\"val/loss\": f\"{loss.item():.4f}\"})\n",
    "            pbar.update(1)\n",
    "\n",
    "            # 获取所有数据上的预测值和真实值,用来验证\n",
    "            all_pred, all_tar, all_loss = accelerator.gather_for_metrics(\n",
    "                (y_pred, y, loss)\n",
    "            )\n",
    "            all_predictions.append(all_pred)\n",
    "            all_targets.append(all_tar)\n",
    "            all_losses.append(all_loss)\n",
    "\n",
    "        val_acc = accuracy(\n",
    "            preds=torch.cat(all_predictions, dim=0),\n",
    "            target=torch.cat(all_targets, dim=0),\n",
    "            task=\"multiclass\",\n",
    "            num_classes=num_classes,\n",
    "        )\n",
    "        val_avg_loss = torch.mean(torch.tensor(all_losses))\n",
    "        pbar.set_postfix({\"val/acc\": f\"{val_acc.item():.4f}\"})\n",
    "\n",
    "    # 本地主进程才保存\n",
    "    if accelerator.is_local_main_process:\n",
    "        # like torch.distributed.barrier, wait for all processes to enter this call.\n",
    "        accelerator.wait_for_everyone()\n",
    "        unwrapped_model: nn.Module = accelerator.unwrap_model(model)\n",
    "        # save\n",
    "        accelerator.save(\n",
    "            obj={\n",
    "                \"model\": unwrapped_model.state_dict(),\n",
    "                \"optimizer\": optimizer.state_dict(),\n",
    "                \"lr_sche\": lr_sche.state_dict(),\n",
    "            },\n",
    "            f=output_dir / \"accelerator.last.pth\",\n",
    "        )\n",
    "accelerator.print(\"training finish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': OrderedDict([('weight',\n",
       "               tensor([[-0.0316, -0.0060, -0.0392,  0.1107,  0.0719,  0.0515,  0.0762,  0.0085,\n",
       "                         0.0586, -0.1150],\n",
       "                       [ 0.0977, -0.0298,  0.1236, -0.1378,  0.1329,  0.1308, -0.0111,  0.1061,\n",
       "                         0.0507, -0.1544],\n",
       "                       [ 0.0623, -0.1914, -0.3090, -0.1462,  0.0033, -0.1074, -0.0763, -0.0846,\n",
       "                         0.0541, -0.1999],\n",
       "                       [ 0.1505,  0.0085,  0.1900, -0.0033,  0.1027,  0.0931, -0.1493,  0.2024,\n",
       "                         0.0396, -0.0359],\n",
       "                       [-0.2538, -0.0443, -0.1529, -0.0206,  0.2259, -0.0468,  0.1679,  0.1371,\n",
       "                        -0.0441,  0.0632]])),\n",
       "              ('bias',\n",
       "               tensor([ 0.0975,  0.0359, -0.1707, -0.1255, -0.0479]))]),\n",
       " 'optimizer': {'state': {0: {'step': tensor(500.),\n",
       "    'exp_avg': tensor([[-9.7995e-03,  5.6651e-03, -2.9284e-03,  3.8839e-03, -1.2792e-02,\n",
       "              1.5966e-02, -7.9051e-04, -2.7192e-03, -3.2121e-03,  1.2161e-03],\n",
       "            [ 7.7052e-03, -1.4970e-02, -1.2600e-03, -1.4953e-03,  4.0345e-06,\n",
       "              4.8517e-03, -2.3221e-04, -1.5730e-02, -5.3769e-03, -5.2220e-03],\n",
       "            [ 5.4523e-03,  6.2075e-03, -2.1817e-03,  4.4775e-03,  6.1500e-03,\n",
       "             -3.0440e-04, -2.4824e-03,  2.3290e-03,  4.3086e-03,  4.2023e-03],\n",
       "            [-4.7727e-03, -5.3276e-03, -1.2861e-02,  4.9050e-03, -2.3012e-03,\n",
       "             -1.1457e-02,  1.2899e-02,  5.3900e-03,  2.1217e-03,  6.1982e-03],\n",
       "            [ 1.4147e-03,  8.4253e-03,  1.9231e-02, -1.1771e-02,  8.9392e-03,\n",
       "             -9.0566e-03, -9.3943e-03,  1.0731e-02,  2.1588e-03, -6.3945e-03]]),\n",
       "    'exp_avg_sq': tensor([[0.0007, 0.0007, 0.0007, 0.0007, 0.0008, 0.0011, 0.0007, 0.0007, 0.0007,\n",
       "             0.0008],\n",
       "            [0.0007, 0.0007, 0.0008, 0.0007, 0.0008, 0.0007, 0.0008, 0.0007, 0.0008,\n",
       "             0.0008],\n",
       "            [0.0007, 0.0009, 0.0007, 0.0006, 0.0008, 0.0007, 0.0006, 0.0007, 0.0008,\n",
       "             0.0008],\n",
       "            [0.0008, 0.0006, 0.0009, 0.0007, 0.0008, 0.0007, 0.0007, 0.0007, 0.0007,\n",
       "             0.0007],\n",
       "            [0.0010, 0.0007, 0.0007, 0.0008, 0.0008, 0.0008, 0.0008, 0.0008, 0.0007,\n",
       "             0.0007]])},\n",
       "   1: {'step': tensor(500.),\n",
       "    'exp_avg': tensor([-0.0002, -0.0074, -0.0011, -0.0020,  0.0108]),\n",
       "    'exp_avg_sq': tensor([0.0008, 0.0007, 0.0007, 0.0007, 0.0007])}},\n",
       "  'param_groups': [{'lr': 0.1,\n",
       "    'betas': (0.9, 0.999),\n",
       "    'eps': 1e-08,\n",
       "    'weight_decay': 0.01,\n",
       "    'amsgrad': False,\n",
       "    'foreach': None,\n",
       "    'maximize': False,\n",
       "    'capturable': False,\n",
       "    'differentiable': False,\n",
       "    'fused': None,\n",
       "    'initial_lr': 0.001,\n",
       "    'params': [0, 1]}]},\n",
       " 'lr_sche': {'T_max': 5,\n",
       "  'eta_min': 0.1,\n",
       "  'base_lrs': [0.001],\n",
       "  'last_epoch': 5,\n",
       "  'verbose': False,\n",
       "  '_step_count': 6,\n",
       "  '_get_lr_called_within_step': False,\n",
       "  '_last_lr': [0.1]}}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.load(output_dir / \"accelerator.last.pth\", map_location=\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save_state和load_state配合使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('accelerator_checkpoint')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 必须为dir\n",
    "accelerator.save_state(output_dir=output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 必须为dir\n",
    "accelerator.load_state(input_dir=output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('weight',\n",
       "              tensor([[-0.0316, -0.0060, -0.0392,  0.1107,  0.0719,  0.0515,  0.0762,  0.0085,\n",
       "                        0.0586, -0.1150],\n",
       "                      [ 0.0977, -0.0298,  0.1236, -0.1378,  0.1329,  0.1308, -0.0111,  0.1061,\n",
       "                        0.0507, -0.1544],\n",
       "                      [ 0.0623, -0.1914, -0.3090, -0.1462,  0.0033, -0.1074, -0.0763, -0.0846,\n",
       "                        0.0541, -0.1999],\n",
       "                      [ 0.1505,  0.0085,  0.1900, -0.0033,  0.1027,  0.0931, -0.1493,  0.2024,\n",
       "                        0.0396, -0.0359],\n",
       "                      [-0.2538, -0.0443, -0.1529, -0.0206,  0.2259, -0.0468,  0.1679,  0.1371,\n",
       "                       -0.0441,  0.0632]], device='cuda:0')),\n",
       "             ('bias',\n",
       "              tensor([ 0.0975,  0.0359, -0.1707, -0.1255, -0.0479], device='cuda:0'))])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.load(output_dir / \"pytorch_model.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'state': {0: {'step': tensor(500.),\n",
       "   'exp_avg': tensor([[-9.7995e-03,  5.6651e-03, -2.9284e-03,  3.8839e-03, -1.2792e-02,\n",
       "             1.5966e-02, -7.9051e-04, -2.7192e-03, -3.2121e-03,  1.2161e-03],\n",
       "           [ 7.7052e-03, -1.4970e-02, -1.2600e-03, -1.4953e-03,  4.0345e-06,\n",
       "             4.8517e-03, -2.3221e-04, -1.5730e-02, -5.3769e-03, -5.2220e-03],\n",
       "           [ 5.4523e-03,  6.2075e-03, -2.1817e-03,  4.4775e-03,  6.1500e-03,\n",
       "            -3.0440e-04, -2.4824e-03,  2.3290e-03,  4.3086e-03,  4.2023e-03],\n",
       "           [-4.7727e-03, -5.3276e-03, -1.2861e-02,  4.9050e-03, -2.3012e-03,\n",
       "            -1.1457e-02,  1.2899e-02,  5.3900e-03,  2.1217e-03,  6.1982e-03],\n",
       "           [ 1.4147e-03,  8.4253e-03,  1.9231e-02, -1.1771e-02,  8.9392e-03,\n",
       "            -9.0566e-03, -9.3943e-03,  1.0731e-02,  2.1588e-03, -6.3945e-03]],\n",
       "          device='cuda:0'),\n",
       "   'exp_avg_sq': tensor([[0.0007, 0.0007, 0.0007, 0.0007, 0.0008, 0.0011, 0.0007, 0.0007, 0.0007,\n",
       "            0.0008],\n",
       "           [0.0007, 0.0007, 0.0008, 0.0007, 0.0008, 0.0007, 0.0008, 0.0007, 0.0008,\n",
       "            0.0008],\n",
       "           [0.0007, 0.0009, 0.0007, 0.0006, 0.0008, 0.0007, 0.0006, 0.0007, 0.0008,\n",
       "            0.0008],\n",
       "           [0.0008, 0.0006, 0.0009, 0.0007, 0.0008, 0.0007, 0.0007, 0.0007, 0.0007,\n",
       "            0.0007],\n",
       "           [0.0010, 0.0007, 0.0007, 0.0008, 0.0008, 0.0008, 0.0008, 0.0008, 0.0007,\n",
       "            0.0007]], device='cuda:0')},\n",
       "  1: {'step': tensor(500.),\n",
       "   'exp_avg': tensor([-0.0002, -0.0074, -0.0011, -0.0020,  0.0108], device='cuda:0'),\n",
       "   'exp_avg_sq': tensor([0.0008, 0.0007, 0.0007, 0.0007, 0.0007], device='cuda:0')}},\n",
       " 'param_groups': [{'lr': 0.1,\n",
       "   'betas': (0.9, 0.999),\n",
       "   'eps': 1e-08,\n",
       "   'weight_decay': 0.01,\n",
       "   'amsgrad': False,\n",
       "   'foreach': None,\n",
       "   'maximize': False,\n",
       "   'capturable': False,\n",
       "   'differentiable': False,\n",
       "   'fused': None,\n",
       "   'initial_lr': 0.001,\n",
       "   'params': [0, 1]}]}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.load(output_dir / \"optimizer.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
