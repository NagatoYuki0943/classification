{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import timm\n",
    "from timm import models\n",
    "from timm.data import resolve_data_config, create_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.9.7'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timm.__version__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 获取模型列表,pretrained=True代表找有预训练模型的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "268"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrain_model = timm.list_models(filter=[\"*vit*\"], exclude_filters=[], pretrained=True)\n",
    "len(pretrain_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['convit_base.fb_in1k',\n",
       " 'convit_small.fb_in1k',\n",
       " 'convit_tiny.fb_in1k',\n",
       " 'crossvit_9_240.in1k',\n",
       " 'crossvit_9_dagger_240.in1k',\n",
       " 'crossvit_15_240.in1k',\n",
       " 'crossvit_15_dagger_240.in1k',\n",
       " 'crossvit_15_dagger_408.in1k',\n",
       " 'crossvit_18_240.in1k',\n",
       " 'crossvit_18_dagger_240.in1k',\n",
       " 'crossvit_18_dagger_408.in1k',\n",
       " 'crossvit_base_240.in1k',\n",
       " 'crossvit_small_240.in1k',\n",
       " 'crossvit_tiny_240.in1k',\n",
       " 'davit_base.msft_in1k',\n",
       " 'davit_small.msft_in1k',\n",
       " 'davit_tiny.msft_in1k',\n",
       " 'efficientvit_b0.r224_in1k',\n",
       " 'efficientvit_b1.r224_in1k',\n",
       " 'efficientvit_b1.r256_in1k',\n",
       " 'efficientvit_b1.r288_in1k',\n",
       " 'efficientvit_b2.r224_in1k',\n",
       " 'efficientvit_b2.r256_in1k',\n",
       " 'efficientvit_b2.r288_in1k',\n",
       " 'efficientvit_b3.r224_in1k',\n",
       " 'efficientvit_b3.r256_in1k',\n",
       " 'efficientvit_b3.r288_in1k',\n",
       " 'efficientvit_m0.r224_in1k',\n",
       " 'efficientvit_m1.r224_in1k',\n",
       " 'efficientvit_m2.r224_in1k',\n",
       " 'efficientvit_m3.r224_in1k',\n",
       " 'efficientvit_m4.r224_in1k',\n",
       " 'efficientvit_m5.r224_in1k',\n",
       " 'fastvit_ma36.apple_dist_in1k',\n",
       " 'fastvit_ma36.apple_in1k',\n",
       " 'fastvit_s12.apple_dist_in1k',\n",
       " 'fastvit_s12.apple_in1k',\n",
       " 'fastvit_sa12.apple_dist_in1k',\n",
       " 'fastvit_sa12.apple_in1k',\n",
       " 'fastvit_sa24.apple_dist_in1k',\n",
       " 'fastvit_sa24.apple_in1k',\n",
       " 'fastvit_sa36.apple_dist_in1k',\n",
       " 'fastvit_sa36.apple_in1k',\n",
       " 'fastvit_t8.apple_dist_in1k',\n",
       " 'fastvit_t8.apple_in1k',\n",
       " 'fastvit_t12.apple_dist_in1k',\n",
       " 'fastvit_t12.apple_in1k',\n",
       " 'flexivit_base.300ep_in1k',\n",
       " 'flexivit_base.300ep_in21k',\n",
       " 'flexivit_base.600ep_in1k',\n",
       " 'flexivit_base.1000ep_in21k',\n",
       " 'flexivit_base.1200ep_in1k',\n",
       " 'flexivit_base.patch16_in21k',\n",
       " 'flexivit_base.patch30_in21k',\n",
       " 'flexivit_large.300ep_in1k',\n",
       " 'flexivit_large.600ep_in1k',\n",
       " 'flexivit_large.1200ep_in1k',\n",
       " 'flexivit_small.300ep_in1k',\n",
       " 'flexivit_small.600ep_in1k',\n",
       " 'flexivit_small.1200ep_in1k',\n",
       " 'gcvit_base.in1k',\n",
       " 'gcvit_small.in1k',\n",
       " 'gcvit_tiny.in1k',\n",
       " 'gcvit_xtiny.in1k',\n",
       " 'gcvit_xxtiny.in1k',\n",
       " 'levit_128.fb_dist_in1k',\n",
       " 'levit_128s.fb_dist_in1k',\n",
       " 'levit_192.fb_dist_in1k',\n",
       " 'levit_256.fb_dist_in1k',\n",
       " 'levit_384.fb_dist_in1k',\n",
       " 'levit_conv_128.fb_dist_in1k',\n",
       " 'levit_conv_128s.fb_dist_in1k',\n",
       " 'levit_conv_192.fb_dist_in1k',\n",
       " 'levit_conv_256.fb_dist_in1k',\n",
       " 'levit_conv_384.fb_dist_in1k',\n",
       " 'maxvit_base_tf_224.in1k',\n",
       " 'maxvit_base_tf_224.in21k',\n",
       " 'maxvit_base_tf_384.in1k',\n",
       " 'maxvit_base_tf_384.in21k_ft_in1k',\n",
       " 'maxvit_base_tf_512.in1k',\n",
       " 'maxvit_base_tf_512.in21k_ft_in1k',\n",
       " 'maxvit_large_tf_224.in1k',\n",
       " 'maxvit_large_tf_224.in21k',\n",
       " 'maxvit_large_tf_384.in1k',\n",
       " 'maxvit_large_tf_384.in21k_ft_in1k',\n",
       " 'maxvit_large_tf_512.in1k',\n",
       " 'maxvit_large_tf_512.in21k_ft_in1k',\n",
       " 'maxvit_nano_rw_256.sw_in1k',\n",
       " 'maxvit_rmlp_base_rw_224.sw_in12k',\n",
       " 'maxvit_rmlp_base_rw_224.sw_in12k_ft_in1k',\n",
       " 'maxvit_rmlp_base_rw_384.sw_in12k_ft_in1k',\n",
       " 'maxvit_rmlp_nano_rw_256.sw_in1k',\n",
       " 'maxvit_rmlp_pico_rw_256.sw_in1k',\n",
       " 'maxvit_rmlp_small_rw_224.sw_in1k',\n",
       " 'maxvit_rmlp_tiny_rw_256.sw_in1k',\n",
       " 'maxvit_small_tf_224.in1k',\n",
       " 'maxvit_small_tf_384.in1k',\n",
       " 'maxvit_small_tf_512.in1k',\n",
       " 'maxvit_tiny_rw_224.sw_in1k',\n",
       " 'maxvit_tiny_tf_224.in1k',\n",
       " 'maxvit_tiny_tf_384.in1k',\n",
       " 'maxvit_tiny_tf_512.in1k',\n",
       " 'maxvit_xlarge_tf_224.in21k',\n",
       " 'maxvit_xlarge_tf_384.in21k_ft_in1k',\n",
       " 'maxvit_xlarge_tf_512.in21k_ft_in1k',\n",
       " 'maxxvit_rmlp_nano_rw_256.sw_in1k',\n",
       " 'maxxvit_rmlp_small_rw_256.sw_in1k',\n",
       " 'maxxvitv2_nano_rw_256.sw_in1k',\n",
       " 'maxxvitv2_rmlp_base_rw_224.sw_in12k',\n",
       " 'maxxvitv2_rmlp_base_rw_224.sw_in12k_ft_in1k',\n",
       " 'maxxvitv2_rmlp_base_rw_384.sw_in12k_ft_in1k',\n",
       " 'mobilevit_s.cvnets_in1k',\n",
       " 'mobilevit_xs.cvnets_in1k',\n",
       " 'mobilevit_xxs.cvnets_in1k',\n",
       " 'mobilevitv2_050.cvnets_in1k',\n",
       " 'mobilevitv2_075.cvnets_in1k',\n",
       " 'mobilevitv2_100.cvnets_in1k',\n",
       " 'mobilevitv2_125.cvnets_in1k',\n",
       " 'mobilevitv2_150.cvnets_in1k',\n",
       " 'mobilevitv2_150.cvnets_in22k_ft_in1k',\n",
       " 'mobilevitv2_150.cvnets_in22k_ft_in1k_384',\n",
       " 'mobilevitv2_175.cvnets_in1k',\n",
       " 'mobilevitv2_175.cvnets_in22k_ft_in1k',\n",
       " 'mobilevitv2_175.cvnets_in22k_ft_in1k_384',\n",
       " 'mobilevitv2_200.cvnets_in1k',\n",
       " 'mobilevitv2_200.cvnets_in22k_ft_in1k',\n",
       " 'mobilevitv2_200.cvnets_in22k_ft_in1k_384',\n",
       " 'mvitv2_base.fb_in1k',\n",
       " 'mvitv2_base_cls.fb_inw21k',\n",
       " 'mvitv2_huge_cls.fb_inw21k',\n",
       " 'mvitv2_large.fb_in1k',\n",
       " 'mvitv2_large_cls.fb_inw21k',\n",
       " 'mvitv2_small.fb_in1k',\n",
       " 'mvitv2_tiny.fb_in1k',\n",
       " 'repvit_m1.dist_in1k',\n",
       " 'repvit_m2.dist_in1k',\n",
       " 'repvit_m3.dist_in1k',\n",
       " 'samvit_base_patch16.sa1b',\n",
       " 'samvit_huge_patch16.sa1b',\n",
       " 'samvit_large_patch16.sa1b',\n",
       " 'tiny_vit_5m_224.dist_in22k',\n",
       " 'tiny_vit_5m_224.dist_in22k_ft_in1k',\n",
       " 'tiny_vit_5m_224.in1k',\n",
       " 'tiny_vit_11m_224.dist_in22k',\n",
       " 'tiny_vit_11m_224.dist_in22k_ft_in1k',\n",
       " 'tiny_vit_11m_224.in1k',\n",
       " 'tiny_vit_21m_224.dist_in22k',\n",
       " 'tiny_vit_21m_224.dist_in22k_ft_in1k',\n",
       " 'tiny_vit_21m_224.in1k',\n",
       " 'tiny_vit_21m_384.dist_in22k_ft_in1k',\n",
       " 'tiny_vit_21m_512.dist_in22k_ft_in1k',\n",
       " 'vit_base_patch8_224.augreg2_in21k_ft_in1k',\n",
       " 'vit_base_patch8_224.augreg_in21k',\n",
       " 'vit_base_patch8_224.augreg_in21k_ft_in1k',\n",
       " 'vit_base_patch8_224.dino',\n",
       " 'vit_base_patch14_dinov2.lvd142m',\n",
       " 'vit_base_patch16_224.augreg2_in21k_ft_in1k',\n",
       " 'vit_base_patch16_224.augreg_in1k',\n",
       " 'vit_base_patch16_224.augreg_in21k',\n",
       " 'vit_base_patch16_224.augreg_in21k_ft_in1k',\n",
       " 'vit_base_patch16_224.dino',\n",
       " 'vit_base_patch16_224.mae',\n",
       " 'vit_base_patch16_224.orig_in21k_ft_in1k',\n",
       " 'vit_base_patch16_224.sam_in1k',\n",
       " 'vit_base_patch16_224_miil.in21k',\n",
       " 'vit_base_patch16_224_miil.in21k_ft_in1k',\n",
       " 'vit_base_patch16_384.augreg_in1k',\n",
       " 'vit_base_patch16_384.augreg_in21k_ft_in1k',\n",
       " 'vit_base_patch16_384.orig_in21k_ft_in1k',\n",
       " 'vit_base_patch16_clip_224.datacompxl',\n",
       " 'vit_base_patch16_clip_224.laion2b',\n",
       " 'vit_base_patch16_clip_224.laion2b_ft_in1k',\n",
       " 'vit_base_patch16_clip_224.laion2b_ft_in12k',\n",
       " 'vit_base_patch16_clip_224.laion2b_ft_in12k_in1k',\n",
       " 'vit_base_patch16_clip_224.openai',\n",
       " 'vit_base_patch16_clip_224.openai_ft_in1k',\n",
       " 'vit_base_patch16_clip_224.openai_ft_in12k',\n",
       " 'vit_base_patch16_clip_224.openai_ft_in12k_in1k',\n",
       " 'vit_base_patch16_clip_384.laion2b_ft_in1k',\n",
       " 'vit_base_patch16_clip_384.laion2b_ft_in12k_in1k',\n",
       " 'vit_base_patch16_clip_384.openai_ft_in1k',\n",
       " 'vit_base_patch16_clip_384.openai_ft_in12k_in1k',\n",
       " 'vit_base_patch16_rpn_224.sw_in1k',\n",
       " 'vit_base_patch32_224.augreg_in1k',\n",
       " 'vit_base_patch32_224.augreg_in21k',\n",
       " 'vit_base_patch32_224.augreg_in21k_ft_in1k',\n",
       " 'vit_base_patch32_224.sam_in1k',\n",
       " 'vit_base_patch32_384.augreg_in1k',\n",
       " 'vit_base_patch32_384.augreg_in21k_ft_in1k',\n",
       " 'vit_base_patch32_clip_224.laion2b',\n",
       " 'vit_base_patch32_clip_224.laion2b_ft_in1k',\n",
       " 'vit_base_patch32_clip_224.laion2b_ft_in12k_in1k',\n",
       " 'vit_base_patch32_clip_224.openai',\n",
       " 'vit_base_patch32_clip_224.openai_ft_in1k',\n",
       " 'vit_base_patch32_clip_384.laion2b_ft_in12k_in1k',\n",
       " 'vit_base_patch32_clip_384.openai_ft_in12k_in1k',\n",
       " 'vit_base_patch32_clip_448.laion2b_ft_in12k_in1k',\n",
       " 'vit_base_r50_s16_224.orig_in21k',\n",
       " 'vit_base_r50_s16_384.orig_in21k_ft_in1k',\n",
       " 'vit_giant_patch14_clip_224.laion2b',\n",
       " 'vit_giant_patch14_dinov2.lvd142m',\n",
       " 'vit_gigantic_patch14_clip_224.laion2b',\n",
       " 'vit_gigantic_patch16_224_ijepa.in22k',\n",
       " 'vit_huge_patch14_224.mae',\n",
       " 'vit_huge_patch14_224.orig_in21k',\n",
       " 'vit_huge_patch14_224_ijepa.in1k',\n",
       " 'vit_huge_patch14_224_ijepa.in22k',\n",
       " 'vit_huge_patch14_clip_224.laion2b',\n",
       " 'vit_huge_patch14_clip_224.laion2b_ft_in1k',\n",
       " 'vit_huge_patch14_clip_224.laion2b_ft_in12k',\n",
       " 'vit_huge_patch14_clip_224.laion2b_ft_in12k_in1k',\n",
       " 'vit_huge_patch14_clip_336.laion2b_ft_in12k_in1k',\n",
       " 'vit_huge_patch16_448_ijepa.in1k',\n",
       " 'vit_large_patch14_clip_224.datacompxl',\n",
       " 'vit_large_patch14_clip_224.laion2b',\n",
       " 'vit_large_patch14_clip_224.laion2b_ft_in1k',\n",
       " 'vit_large_patch14_clip_224.laion2b_ft_in12k',\n",
       " 'vit_large_patch14_clip_224.laion2b_ft_in12k_in1k',\n",
       " 'vit_large_patch14_clip_224.openai',\n",
       " 'vit_large_patch14_clip_224.openai_ft_in1k',\n",
       " 'vit_large_patch14_clip_224.openai_ft_in12k',\n",
       " 'vit_large_patch14_clip_224.openai_ft_in12k_in1k',\n",
       " 'vit_large_patch14_clip_336.laion2b_ft_in1k',\n",
       " 'vit_large_patch14_clip_336.laion2b_ft_in12k_in1k',\n",
       " 'vit_large_patch14_clip_336.openai',\n",
       " 'vit_large_patch14_clip_336.openai_ft_in12k_in1k',\n",
       " 'vit_large_patch14_dinov2.lvd142m',\n",
       " 'vit_large_patch16_224.augreg_in21k',\n",
       " 'vit_large_patch16_224.augreg_in21k_ft_in1k',\n",
       " 'vit_large_patch16_224.mae',\n",
       " 'vit_large_patch16_384.augreg_in21k_ft_in1k',\n",
       " 'vit_large_patch32_224.orig_in21k',\n",
       " 'vit_large_patch32_384.orig_in21k_ft_in1k',\n",
       " 'vit_large_r50_s32_224.augreg_in21k',\n",
       " 'vit_large_r50_s32_224.augreg_in21k_ft_in1k',\n",
       " 'vit_large_r50_s32_384.augreg_in21k_ft_in1k',\n",
       " 'vit_medium_patch16_gap_240.sw_in12k',\n",
       " 'vit_medium_patch16_gap_256.sw_in12k_ft_in1k',\n",
       " 'vit_medium_patch16_gap_384.sw_in12k_ft_in1k',\n",
       " 'vit_relpos_base_patch16_224.sw_in1k',\n",
       " 'vit_relpos_base_patch16_clsgap_224.sw_in1k',\n",
       " 'vit_relpos_base_patch32_plus_rpn_256.sw_in1k',\n",
       " 'vit_relpos_medium_patch16_224.sw_in1k',\n",
       " 'vit_relpos_medium_patch16_cls_224.sw_in1k',\n",
       " 'vit_relpos_medium_patch16_rpn_224.sw_in1k',\n",
       " 'vit_relpos_small_patch16_224.sw_in1k',\n",
       " 'vit_small_patch8_224.dino',\n",
       " 'vit_small_patch14_dinov2.lvd142m',\n",
       " 'vit_small_patch16_224.augreg_in1k',\n",
       " 'vit_small_patch16_224.augreg_in21k',\n",
       " 'vit_small_patch16_224.augreg_in21k_ft_in1k',\n",
       " 'vit_small_patch16_224.dino',\n",
       " 'vit_small_patch16_384.augreg_in1k',\n",
       " 'vit_small_patch16_384.augreg_in21k_ft_in1k',\n",
       " 'vit_small_patch32_224.augreg_in21k',\n",
       " 'vit_small_patch32_224.augreg_in21k_ft_in1k',\n",
       " 'vit_small_patch32_384.augreg_in21k_ft_in1k',\n",
       " 'vit_small_r26_s32_224.augreg_in21k',\n",
       " 'vit_small_r26_s32_224.augreg_in21k_ft_in1k',\n",
       " 'vit_small_r26_s32_384.augreg_in21k_ft_in1k',\n",
       " 'vit_srelpos_medium_patch16_224.sw_in1k',\n",
       " 'vit_srelpos_small_patch16_224.sw_in1k',\n",
       " 'vit_tiny_patch16_224.augreg_in21k',\n",
       " 'vit_tiny_patch16_224.augreg_in21k_ft_in1k',\n",
       " 'vit_tiny_patch16_384.augreg_in21k_ft_in1k',\n",
       " 'vit_tiny_r_s16_p8_224.augreg_in21k',\n",
       " 'vit_tiny_r_s16_p8_224.augreg_in21k_ft_in1k',\n",
       " 'vit_tiny_r_s16_p8_384.augreg_in21k_ft_in1k']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrain_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = timm.create_model(\"vit_base_patch16_224\", pretrained=False, num_classes=5)      # 默认权重\n",
    "model = timm.create_model(\"vit_base_patch16_224.mae\", pretrained=False, num_classes=5)  # 选择权重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\miniconda3\\envs\\pytorch\\Lib\\site-packages\\torch\\functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ..\\aten\\src\\ATen\\native\\TensorShape.cpp:3484.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "model = timm.create_model(\"maxvit_nano_rw_256\", pretrained=False, num_classes=5)\n",
    "model = models.maxxvit.maxvit_nano_rw_256(pretrained=False, num_classes=5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 创建对应的图片预处理，配合PIL.Image.Open('path').convert('RGB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_size': (3, 256, 256),\n",
       " 'interpolation': 'bicubic',\n",
       " 'mean': (0.5, 0.5, 0.5),\n",
       " 'std': (0.5, 0.5, 0.5),\n",
       " 'crop_pct': 0.95,\n",
       " 'crop_mode': 'center'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = resolve_data_config({}, model=model)\n",
    "config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Compose(\n",
       "    Resize(size=269, interpolation=bicubic, max_size=None, antialias=warn)\n",
       "    CenterCrop(size=(256, 256))\n",
       "    ToTensor()\n",
       "    Normalize(mean=tensor([0.5000, 0.5000, 0.5000]), std=tensor([0.5000, 0.5000, 0.5000]))\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_transform(**config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(1, 3, 256, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5])\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.inference_mode():\n",
    "    y = model(x)\n",
    "print(y.size())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 获取特征"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建一个没有池化和分类层的模型(池化之前的特征)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512, 8, 8])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = timm.create_model('maxvit_nano_rw_256', pretrained=False, num_classes=0, global_pool='')\n",
    "model.eval()\n",
    "with torch.inference_mode():\n",
    "    y = model(x)\n",
    "y.size()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建一个没有分类层的模型(池化之后的特征)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = timm.create_model('maxvit_nano_rw_256', pretrained=False, num_classes=0)\n",
    "model.eval()\n",
    "with torch.inference_mode():\n",
    "    y = model(x)\n",
    "y.size()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 多尺度特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[64, 64, 128, 256, 512]\n",
      "[2, 4, 8, 16, 32]\n",
      "torch.Size([1, 64, 128, 128])\n",
      "torch.Size([1, 64, 64, 64])\n",
      "torch.Size([1, 128, 32, 32])\n",
      "torch.Size([1, 256, 16, 16])\n",
      "torch.Size([1, 512, 8, 8])\n"
     ]
    }
   ],
   "source": [
    "model = timm.create_model('maxvit_nano_rw_256', pretrained=False, features_only=True)\n",
    "model.eval()\n",
    "with torch.inference_mode():\n",
    "    y = model(x)\n",
    "# 获取返回的通道数\n",
    "print(model.feature_info.channels())    # [64, 256, 512, 1024, 2048]\n",
    "print(model.feature_info.reduction())   # [2, 4, 8, 16, 32]\n",
    "for layer in y:\n",
    "    print(layer.size())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "多尺度特征\n",
    "\n",
    "可以选择特定的特征图级别（out_indices）或限制步幅（output_stride）：\n",
    "\n",
    "out_indices:    选择输出哪个索引。指定返回哪个feature maps to return, 从0开始，out_indices[i]对应着 C(i + 1) feature level。\n",
    "\n",
    "output_stride:  限制网络的特征输出步幅(也适用于分类模式)。通过dilated convolutions控制网络的output stride。大多数网络默认 stride 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[128, 256, 512]\n",
      "[8, 16, 32]\n",
      "torch.Size([1, 128, 32, 32])\n",
      "torch.Size([1, 256, 16, 16])\n",
      "torch.Size([1, 512, 8, 8])\n"
     ]
    }
   ],
   "source": [
    "model = timm.create_model('maxvit_nano_rw_256', pretrained=False, features_only=True, out_indices=[2, 3, 4]) # 选择后3层\n",
    "model.eval()\n",
    "with torch.inference_mode():\n",
    "    y = model(x)\n",
    "# 获取返回的通道数\n",
    "print(model.feature_info.channels())    # [512, 1024, 2048]\n",
    "print(model.feature_info.reduction())   # [8, 16, 32]\n",
    "for layer in y:\n",
    "    print(layer.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
